-- name: test_event_scheduler
set enable_pipeline_event_scheduler=true;
set enable_group_execution=false;
set enable_per_bucket_optimize=false;

-- basic scan test:
CREATE TABLE `t0` (
  `c0` int DEFAULT NULL,
  `c1` bigint DEFAULT NULL,
  `c2` string DEFAULT NULL
) ENGINE=OLAP
DUPLICATE KEY(`c0`)
COMMENT "OLAP"
DISTRIBUTED BY HASH(`c0`) BUCKETS 1
PROPERTIES (
"replication_num" = "1"
);
-- 
set pipeline_dop = 1;
set io_tasks_per_scan_operator=1;

insert into t0 SELECT generate_series, generate_series, generate_series FROM TABLE(generate_series(1,  4));
select * from t0;
insert into t0 SELECT generate_series, generate_series, generate_series FROM TABLE(generate_series(1,  409600));
-- scan has output always false
select c0 from t0 where c0 is null;

-- simple agg without local-exchange once stage agg
select count(c1) from t0;

-- multi-scan tasks
CREATE TABLE `t1` (
  `c0` int DEFAULT NULL,
  `c1` bigint DEFAULT NULL,
  `c2` string DEFAULT NULL
) ENGINE=OLAP
DUPLICATE KEY(`c0`)
COMMENT "OLAP"
DISTRIBUTED BY HASH(`c0`) BUCKETS 4
PROPERTIES (
"replication_num" = "1"
);
set io_tasks_per_scan_operator=4;
insert into t1 SELECT generate_series, generate_series, generate_series FROM TABLE(generate_series(1,  409600));
-- simple with local-exchange, two stage block agg
select count(c1) from t1;
-- simple with local-exchange, two stage streaming distinct agg
insert into blackhole() select distinct c1,c2,c0 from t1;
-- simple with local-exchange, two stage streaming agg
insert into blackhole() select c2,sum(c0),c1 from t1 group by c1,c2;

set pipeline_dop=2;
-- test with shared morsel queue
select count(c1) from t1;

-- test exchange with limit
select count(*) from (select * from t1 limit 1000) t;

-- test with force streaming
set streaming_preaggregation_mode="force_streaming";
insert into blackhole() select distinct c1,c2,c0 from t1;

-- test with force streaming
set streaming_preaggregation_mode="force_streaming";
insert into blackhole() select sum(c0),c1,c2 from t1 group by c1, c2;

-- test basic scan with OUTPUT_FULL
set query_debug_options = '{"execDebugOptions":[{"plan_node_id":0, "debug_action":"WAIT", "value":1000}]}';
insert into blackhole() select * from t1;
-- test streaming agg with OUTPUT_FULL
set query_debug_options = '{"execDebugOptions":[{"plan_node_id":1, "debug_action":"WAIT", "value":1000}]}';
set streaming_preaggregation_mode="force_streaming";
insert into blackhole() select distinct c1,c2 from t1;
set query_debug_options = '';

-- test basic hash join without RF
select sum(lc0),sum(rc1) from (select l.c0 lc0,r.c1 rc1 from t1 l left join t1 r on l.c0 = r.c0) t;
-- test basic shuffle join with GRF 
select sum(lc0),sum(rc1) from (select l.c0 lc0,r.c1 rc1 from t1 l join [shuffle] t1 r on l.c0 = r.c0) t;
-- test basic hash join with RF
select sum(lc0),sum(rc1) from (select l.c0 lc0,r.c1 rc1 from t1 l join t1 r on l.c0 = r.c0 where r.c2 < 500) t;
-- test top n with RF
select count(*) from (select * from t1 order by c0 limit 10)t;
select count(*), sum(c0) from (select c0 from t1 group by c0 order by c0 limit 10)t;
-- test cross join
select count(*), sum(lc0), sum(rc0) from (select l.c0 lc0, r.c0 rc0 from t1 l join t0 r where l.c0 < 5) t;
-- test cross join with RF
select count(*), sum(lc0), sum(rc0) from (select l.c0 lc0, r.c0 rc0 from t1 l join t0 r where r.c0 <= 1 and l.c1 <= r.c1) t;
-- test broadcast join
select sum(lc0),sum(rc1) from (select l.c0 lc0,r.c1 rc1 from t1 l join[broadcast] t1 r on l.c0 = r.c0 where r.c2 < 10) t;
-- test colocate join (self colocate join)
select count(*), sum(lc0),sum(rc1) from (select l.c0 lc0,r.c1 rc1 from t1 l join t1 r on l.c0 = r.c0) t;
-- test colocate agg
set enable_group_execution = true;
select count(*), sum(c0) from (select c0 from t1 group by c0)t;
-- test bucket agg
set enable_per_bucket_optimize=true;
set pipeline_dop=1;
select count(*), sum(c0) from (select c0 from t1 group by c0 order by c0 limit 10)t;
-- test sort
set enable_parallel_merge=false;
select c0 from t0 order by c0 limit 3;
-- test parallel merge
set enable_parallel_merge=true;
select c0 from t0 order by c0 limit 3;
-- test union cases
select count(), min(x), max(x), min(y), max(y) from (select 1 x, "val" y union all select c0 x, c1 y from t0) t;
-- agg union
select count(), min(x), max(x), min(y), max(y) from (select 1 x, "val" y union select c0 x, c1 y from t0) t;
-- test intersect
select count(*), sum(c0) from (select c0 from t0 where c1 >= 1 intersect select c0 from t0 where c0 < 1) t;
-- test except
select count(*), sum(c0) from (select c0 from t0 where c1 >= 1 except select c0 from t0 where c0 < 1) t;
-- test window operator
select count(*), sum(rk) from (select c1, rank() over(partition by c0) rk from (select c0,c1 from t1 order by 1,2 limit 10)t) t;
-- test local-partition topn
select * from (select c1, rk from (select c1, rank() over(partition by c0) rk from (select c0,c1 from t1)t)t)t where rk <=2  order by 2,1 limit 3;
-- analytic
select sum(rk) from (select c1, rank() over(partition by c0) rk from t0) t;
-- test with no spill CTE
with agged_table as ( select distinct c0, c1 from t0) select /*+SET_VAR(cbo_cte_reuse_rate=0) */ count(*) ,sum(l.c0), sum(r.c0), sum(l.c1), sum(r.c1) from agged_table l join [colocate] agged_table r on l.c0 = r.c0 and l.c1 = r.c1;
-- TODO external scan